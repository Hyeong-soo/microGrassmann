"""
trace_g.py — 기하학 벡터 g가 만들어지는 과정을 숫자로 추적하기
================================================================
"g가 뭔데?" 에 대한 답:
  g = 이전 토큰들과의 "기하학적 관계"를 요약한 벡터

이 파일은 "hello"에서 위치 4의 'o'가
이전 글자들과 어떤 기하학적 관계를 맺는지,
숫자 하나하나를 추적합니다.

실행: python3 trace_g.py
================================================================
"""

import random
import math
random.seed(42)

def 구분선(title):
    print(f"\n{'='*64}")
    print(f"  {title}")
    print(f"{'='*64}\n")

def fmt(vec, n=3):
    """벡터를 보기 좋게 포맷"""
    return "[" + ", ".join(f"{v:+.{n}f}" for v in vec) + "]"

def mat_vec(W, x):
    """행렬-벡터 곱"""
    return [sum(wi*xi for wi, xi in zip(row, x)) for row in W]


# ================================================================
구분선("설정: 'hello'를 처리하는 중, 현재 위치는 'o' (위치 4)")
# ================================================================

print("""
  시퀀스: h  e  l  l  o
  위치:   0  1  2  3  4 ← 현재

  window = [1, 2, 4] 이므로 'o'가 참조하는 이전 토큰:
    delta=1 → 위치 3의 'l'  (바로 앞)
    delta=2 → 위치 2의 'l'  (두 칸 앞)
    delta=4 → 위치 0의 'h'  (네 칸 앞)

  목표: 이 3개의 관계에서 기하학 벡터 g를 만들어
        현재 'o'의 표현을 풍부하게 하는 것
""")

# 임베딩 차원 d=6 (이해를 위해 작게), 축소 차원 r=3
d = 6
r = 3
plucker_dim = r * (r - 1) // 2  # C(3,2) = 3

print(f"  임베딩 차원 d={d}, 축소 차원 r={r}, Plucker 차원={plucker_dim}")


# ================================================================
구분선("STEP 0: 각 글자의 임베딩 벡터 (x)")
# ================================================================

print("""
  실제 모델에서는 학습으로 결정되지만,
  여기서는 고정된 예시 벡터를 사용합니다.
  각 벡터는 d=6 차원입니다.
""")

embeddings = {
    'h': [ 0.5, -0.3,  0.8,  0.2, -0.1,  0.4],
    'e': [-0.2,  0.7, -0.1,  0.5,  0.3, -0.6],
    'l': [ 0.3,  0.4,  0.6, -0.2,  0.5,  0.1],  # 위치2
    'l2':[ 0.3,  0.4,  0.6, -0.2,  0.5,  0.1],  # 위치3 (같은 글자)
    'o': [ 0.1, -0.5,  0.3,  0.9, -0.4,  0.7],
}

for ch, vec in embeddings.items():
    name = ch if ch != 'l2' else 'l'
    pos = {'h':0, 'e':1, 'l':2, 'l2':3, 'o':4}[ch]
    marker = " ← 현재" if ch == 'o' else ""
    print(f"  위치 {pos} '{name}':  x = {fmt(vec)}{marker}")


# ================================================================
구분선("STEP 1: 차원 축소 — x(6차원) → z(3차원)")
# ================================================================

print("""
  W_red (3x6 행렬)로 각 토큰의 벡터를 압축합니다.
  z = W_red @ x

  왜 줄이나?
    Plucker 계산에서 모든 i<j 쌍을 봐야 하는데,
    6차원이면 C(6,2)=15개 쌍 → 너무 많음
    3차원이면 C(3,2)=3개 쌍 → 관리 가능
""")

W_red = [
    [ 0.4, -0.2,  0.3,  0.1, -0.5,  0.2],
    [ 0.1,  0.5, -0.3,  0.4,  0.2, -0.1],
    [-0.3,  0.1,  0.6, -0.2,  0.3,  0.4],
]

z_all = {}
for ch in ['h', 'e', 'l', 'l2', 'o']:
    z = mat_vec(W_red, embeddings[ch])
    z_all[ch] = z
    name = ch if ch != 'l2' else 'l'
    pos = {'h':0, 'e':1, 'l':2, 'l2':3, 'o':4}[ch]
    print(f"  위치 {pos} '{name}':  x{fmt(embeddings[ch][:3])}... → z={fmt(z)}")

z_o = z_all['o']
print(f"\n  현재 'o'의 축소 벡터: z_o = {fmt(z_o)}")
print(f"  이 z_o를 이전 토큰들의 z와 비교할 겁니다.")


# ================================================================
구분선("STEP 2: Plucker 좌표 계산 — 각 이전 토큰과의 '관계'")
# ================================================================

print("""
  이제 핵심입니다!
  z_o (현재 'o')와 각 이전 토큰의 z 사이에서
  Plucker 좌표를 계산합니다.

  r=3이므로 Plucker는 3개의 성분:
    p_01 = z_o[0]*z_prev[1] - z_o[1]*z_prev[0]  (xy 그림자)
    p_02 = z_o[0]*z_prev[2] - z_o[2]*z_prev[0]  (xz 그림자)
    p_12 = z_o[1]*z_prev[2] - z_o[2]*z_prev[1]  (yz 그림자)

  각 Plucker 벡터 = "현재 토큰과 이전 토큰이
  3차원 공간에서 어떤 평면을 이루는가?"
""")

window = [1, 2, 4]
prev_map = {1: ('l2', 3, 'l'), 2: ('l', 2, 'l'), 4: ('h', 0, 'h')}

plucker_results = {}

for delta in window:
    ch_key, pos, ch_name = prev_map[delta]
    z_prev = z_all[ch_key]

    print(f"  --- delta={delta}: 'o'(위치4)와 '{ch_name}'(위치{pos}) ---\n")
    print(f"    z_o    = {fmt(z_o)}")
    print(f"    z_prev = {fmt(z_prev)}")

    p01 = z_o[0]*z_prev[1] - z_o[1]*z_prev[0]
    p02 = z_o[0]*z_prev[2] - z_o[2]*z_prev[0]
    p12 = z_o[1]*z_prev[2] - z_o[2]*z_prev[1]
    plucker = [p01, p02, p12]

    print(f"\n    Plucker 계산:")
    print(f"      p_01 = {z_o[0]:+.3f}*{z_prev[1]:+.3f} - {z_o[1]:+.3f}*{z_prev[0]:+.3f} = {p01:+.4f}  (xy 그림자)")
    print(f"      p_02 = {z_o[0]:+.3f}*{z_prev[2]:+.3f} - {z_o[2]:+.3f}*{z_prev[0]:+.3f} = {p02:+.4f}  (xz 그림자)")
    print(f"      p_12 = {z_o[1]:+.3f}*{z_prev[2]:+.3f} - {z_o[2]:+.3f}*{z_prev[1]:+.3f} = {p12:+.4f}  (yz 그림자)")
    print(f"\n    Plucker = {fmt(plucker, 4)}")

    # 정규화
    norm = math.sqrt(sum(p*p for p in plucker) + 1e-8)
    plucker_normed = [p / norm for p in plucker]
    print(f"    정규화 (크기=1로): {fmt(plucker_normed, 4)}")
    print(f"    → '방향'만 남기고 '크기'는 버림 (어떤 평면인지만 중요)")

    plucker_results[delta] = plucker_normed
    print()


# ================================================================
구분선("STEP 3: Plucker → 모델 차원으로 복원 (g_delta)")
# ================================================================

print("""
  Plucker 좌표는 3차원이지만, 모델의 벡터는 6차원입니다.
  W_plu (6x3 행렬)로 3차원 Plucker를 6차원으로 변환합니다.

  g_delta = W_plu @ plucker_normalized

  이것이 하는 일:
    "이 두 토큰이 이런 기하학적 관계입니다"라는 3차원 정보를
    모델이 이해할 수 있는 6차원 표현으로 번역하는 것.

  비유: 프랑스어(3차원) → 한국어(6차원) 번역기
""")

W_plu = [
    [ 0.3, -0.4,  0.2],
    [ 0.5,  0.1, -0.3],
    [-0.2,  0.6,  0.4],
    [ 0.1, -0.3,  0.5],
    [ 0.4,  0.2, -0.1],
    [-0.1,  0.3,  0.6],
]

g_deltas = {}

for delta in window:
    plucker_n = plucker_results[delta]
    g_delta = mat_vec(W_plu, plucker_n)
    g_deltas[delta] = g_delta

    ch_key, pos, ch_name = prev_map[delta]
    print(f"  delta={delta} ('{ch_name}'과의 관계):")
    print(f"    Plucker(3D) {fmt(plucker_n, 3)}")
    print(f"    → W_plu로 변환 →")
    print(f"    g_delta(6D) {fmt(g_delta, 3)}")
    print(f"    이것 = \"'{ch_name}'와의 기하학적 관계를 6차원으로 표현한 것\"")
    print()


# ================================================================
구분선("STEP 4: g_delta들의 평균 → 최종 기하학 벡터 g")
# ================================================================

print("""
  3개의 이전 토큰과의 관계를 각각 구했습니다:
    g_delta1 = 'l'(위치3)과의 관계  (바로 앞, 가장 가까움)
    g_delta2 = 'l'(위치2)과의 관계  (두 칸 앞)
    g_delta4 = 'h'(위치0)과의 관계  (네 칸 앞, 가장 먼 문맥)

  이 3개를 평균내서 하나의 벡터 g로 만듭니다:
    g = (g_delta1 + g_delta2 + g_delta4) / 3
""")

print("  각 delta의 기여:")
for delta in window:
    ch_key, pos, ch_name = prev_map[delta]
    print(f"    delta={delta} ('{ch_name}'): {fmt(g_deltas[delta], 3)}")

# 평균 계산
g = [0.0] * d
for delta in window:
    for j in range(d):
        g[j] += g_deltas[delta][j]
for j in range(d):
    g[j] /= len(window)

print(f"\n  평균 →  g = {fmt(g, 4)}")
print(f"""
  ┌──────────────────────────────────────────────────────────┐
  │                                                          │
  │  g = "현재 'o'가 주변 토큰들과 맺는                       │
  │       기하학적 관계의 요약"                                │
  │                                                          │
  │  g의 각 차원은:                                           │
  │    - 인접 문맥('l', delta=1)에서 온 정보                   │
  │    - 약간 넓은 문맥('l', delta=2)에서 온 정보              │
  │    - 먼 문맥('h', delta=4)에서 온 정보                     │
  │  이 세 가지가 혼합된 것입니다.                              │
  │                                                          │
  └──────────────────────────────────────────────────────────┘
""")


# ================================================================
구분선("STEP 5: x vs g — 각각 무엇을 알고 있나?")
# ================================================================

x_o = embeddings['o']

print(f"  x (원본 벡터):  {fmt(x_o, 3)}")
print(f"  g (기하학 벡터): {fmt(g, 3)}")
print()
print("""
  x가 아는 것:
    "나는 'o'야. 나의 고유한 특성은 이거야."
    → 현재 토큰 자체의 정보 (독립적, 문맥 무관)

  g가 아는 것:
    "앞에 'l', 'l', 'h'가 있었고,
     나('o')와 그들의 기하학적 관계는 이렇더라."
    → 주변 토큰과의 관계 정보 (문맥 의존적)

  비유:
    x = 이력서 (나 자신의 스펙)
    g = 추천서 (주변 사람들이 나를 어떻게 보는지)

    둘 다 있어야 "나"를 완전하게 표현할 수 있습니다!
""")


# ================================================================
구분선("STEP 6: 게이트 혼합 — x와 g를 차원별로 섞기")
# ================================================================

print("""
  alpha = sigmoid(W_gate_h @ x + W_gate_g @ g)
  output = alpha * x + (1 - alpha) * g

  alpha가 차원별로 "x와 g 중 뭘 더 쓸지" 결정합니다.
""")

# 간단한 게이트 가중치 (예시)
alpha_example = [0.8, 0.3, 0.6, 0.2, 0.9, 0.5]

print(f"  x (원본):  {fmt(x_o, 3)}")
print(f"  g (기하학): {fmt(g, 3)}")
print(f"  alpha:     {fmt(alpha_example, 1)}")
print()

output = [0.0] * d
print(f"  {'차원':>4}  {'alpha':>6}  {'alpha*x':>9}  {'(1-a)*g':>9}  {'= output':>9}  판단")
print(f"  {'─'*64}")

for j in range(d):
    a = alpha_example[j]
    ax = a * x_o[j]
    ag = (1 - a) * g[j]
    out = ax + ag
    output[j] = out
    verdict = "원본 x 위주" if a >= 0.5 else "기하학 g 위주"
    print(f"  [{j}]   {a:.1f}    {ax:+.4f}    {ag:+.4f}    {out:+.4f}   {verdict}")

print(f"\n  최종 output = {fmt(output, 4)}")
print(f"""
  결과 해석:
    차원 0: alpha=0.8 → 원본 80% + 기하학 20% (이 차원은 'o' 자체 정보가 충분)
    차원 1: alpha=0.3 → 원본 30% + 기하학 70% (이 차원은 주변 관계가 중요)
    차원 3: alpha=0.2 → 원본 20% + 기하학 80% (이 차원은 문맥이 매우 중요)
    차원 4: alpha=0.9 → 원본 90% + 기하학 10% (이 차원은 'o' 고유 특성이 핵심)
""")


# ================================================================
구분선("STEP 7: Attention과의 전체 비교")
# ================================================================

print("""
  같은 상황: "hello"에서 위치 4의 'o'가 문맥을 파악하는 방법

  ┌─── Attention의 방법 ──────────────────────────────────────┐
  │                                                           │
  │  'o'에서 Query Q를 만들고,                                 │
  │  'h','e','l','l' 각각에서 Key K를 만들어서                  │
  │  Q.K 내적으로 유사도 점수를 구함:                            │
  │                                                           │
  │    'h'와의 점수: 0.12  → 가중치 0.14                       │
  │    'e'와의 점수: 0.08  → 가중치 0.12                       │
  │    'l'와의 점수: 0.35  → 가중치 0.31                       │
  │    'l'와의 점수: 0.42  → 가중치 0.34  ← 가장 높음          │
  │                                   (합 = 1.0)              │
  │                                                           │
  │  → 4개 토큰의 Value를 위 가중치로 가중 평균                  │
  │  → 4번의 내적 계산 (모든 이전 토큰과)                       │
  │  → 각 관계 = 스칼라 1개                                    │
  └───────────────────────────────────────────────────────────┘

  ┌─── Grassmann의 방법 ──────────────────────────────────────┐
  │                                                           │
  │  'o'와 가까운 3개만 봄 (delta=1,2,4):                      │
  │                                                           │
  │    'l'(위치3)과 Plucker → {fmt(plucker_results[1], 3):>22} │
  │    'l'(위치2)과 Plucker → {fmt(plucker_results[2], 3):>22} │
  │    'h'(위치0)과 Plucker → {fmt(plucker_results[4], 3):>22} │
  │                                                           │
  │  각각 W_plu로 6차원 변환 후 평균 → g                       │
  │  게이트로 x와 g를 차원별 혼합                               │
  │                                                           │
  │  → 3번의 Plucker 계산 (고정된 윈도우만)                     │
  │  → 각 관계 = 벡터 (스칼라 1개가 아니라 3개!)               │
  └───────────────────────────────────────────────────────────┘

  핵심 차이:
    Attention:  적은 정보(스칼라) x 많은 토큰(전부) = 넓고 얕게
    Grassmann:  풍부한 정보(벡터) x 적은 토큰(3개) = 좁고 깊게
""")


# ================================================================
구분선("요약: g란 무엇인가?")
# ================================================================

print("""
  g는 "주변 토큰들이 나에 대해 해주는 이야기"입니다.

  만들어지는 과정:

    x(6D) ──W_red──→ z(3D)          차원 축소
                       │
                       ├── z_prev(delta=1)와 Plucker → [3D 관계] ──W_plu──→ g1(6D)
                       ├── z_prev(delta=2)와 Plucker → [3D 관계] ──W_plu──→ g2(6D)
                       └── z_prev(delta=4)와 Plucker → [3D 관계] ──W_plu──→ g3(6D)
                                                                        │
                                                           평균 ←───────┘
                                                            │
                                                            ↓
                                                       g(6D): 기하학 벡터
                                                            │
                                    x ──→ gate(alpha) ←── g │
                                           │                 │
                                           ↓                 │
                                     output = alpha*x + (1-alpha)*g


  g의 각 차원에 담긴 것:
    "가까운 문맥(delta=1)의 관계"  ─┐
    "중간 문맥(delta=2)의 관계"    ─┼─→ 평균되어 혼합됨
    "먼 문맥(delta=4)의 관계"      ─┘

  g가 없으면: x만으로 예측 → "나는 'o'야" (문맥 무시)
  g가 있으면: x+g로 예측   → "나는 'h-e-l-l' 뒤에 오는 'o'야" (문맥 반영)
""")
